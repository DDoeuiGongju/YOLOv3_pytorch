{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.transforms\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def horisontal_flip(images, targets):\n",
    "    images = torch.flip(images, [-1])\n",
    "    targets[:, 2] = 1 - targets[:, 2]\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "def pad_to_square(image, pad_value=0):\n",
    "    _, h, w = image.shape\n",
    "\n",
    "    # 너비와 높이의 차\n",
    "    difference = abs(h - w)\n",
    "\n",
    "    # (top, bottom) padding or (left, right) padding\n",
    "    if h <= w:\n",
    "        top = difference // 2\n",
    "        bottom = difference - difference // 2\n",
    "        pad = [0, 0, top, bottom]\n",
    "    else:\n",
    "        left = difference // 2\n",
    "        right = difference - difference // 2\n",
    "        pad = [left, right, 0, 0]\n",
    "\n",
    "    # Add padding\n",
    "    image = F.pad(image, pad, mode='constant', value=pad_value)\n",
    "    return image, pad\n",
    "\n",
    "\n",
    "def resize(image, size):\n",
    "    return F.interpolate(image.unsqueeze(0), size, mode='bilinear', align_corners=True).squeeze(0)\n",
    "\n",
    "\n",
    "class ImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path, image_size):\n",
    "        self.image_files = sorted(glob.glob(\"{}/*.*\".format(folder_path)))\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_files[index]\n",
    "\n",
    "        # Extract image as PyTorch tensor\n",
    "        image = torchvision.transforms.ToTensor()(Image.open(image_path).convert('RGB'))\n",
    "\n",
    "        # Pad to square resolution\n",
    "        image, _ = pad_to_square(image)\n",
    "\n",
    "        # Resize\n",
    "        image = resize(image, self.image_size)\n",
    "        return image_path, image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "\n",
    "class ListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_path: str, image_size: int, augment: bool, multiscale: bool, normalized_labels=True):\n",
    "        with open(list_path, 'r') as file:\n",
    "            self.image_files = file.readlines()\n",
    "\n",
    "        self.label_files = [path.replace('images', 'labels').replace('.png', '.txt').replace('.jpg', '.txt')\n",
    "                                .replace('JPEGImages', 'labels') for path in self.image_files]\n",
    "        self.image_size = image_size\n",
    "        self.max_objects = 100\n",
    "        self.augment = augment\n",
    "        self.multiscale = multiscale\n",
    "        self.normalized_labels = normalized_labels\n",
    "        self.batch_count = 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Image\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        image_path = self.image_files[index].rstrip()\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "            transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ColorJitter(brightness=1.5, saturation=1.5, hue=0.1),\n",
    "                torchvision.transforms.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            transforms = torchvision.transforms.ToTensor()\n",
    "\n",
    "        # Extract image as PyTorch tensor\n",
    "        image = transforms(Image.open(image_path).convert('RGB'))\n",
    "\n",
    "        _, h, w = image.shape\n",
    "        h_factor, w_factor = (h, w) if self.normalized_labels else (1, 1)\n",
    "\n",
    "        # Pad to square resolution\n",
    "        image, pad = pad_to_square(image)\n",
    "        _, padded_h, padded_w = image.shape\n",
    "\n",
    "        # 2. Label\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        label_path = self.label_files[index].rstrip()\n",
    "\n",
    "        targets = None\n",
    "        if os.path.exists(label_path):\n",
    "            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n",
    "\n",
    "            # Extract coordinates for unpadded + unscaled image\n",
    "            x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\n",
    "            y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\n",
    "            x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\n",
    "            y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\n",
    "\n",
    "            # Adjust for added padding\n",
    "            x1 += pad[0]\n",
    "            y1 += pad[2]\n",
    "            x2 += pad[1]\n",
    "            y2 += pad[3]\n",
    "\n",
    "            # Returns (x, y, w, h)\n",
    "            boxes[:, 1] = ((x1 + x2) / 2) / padded_w\n",
    "            boxes[:, 2] = ((y1 + y2) / 2) / padded_h\n",
    "            boxes[:, 3] *= w_factor / padded_w\n",
    "            boxes[:, 4] *= h_factor / padded_h\n",
    "\n",
    "            targets = torch.zeros((len(boxes), 6))\n",
    "            targets[:, 1:] = boxes\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "            if np.random.random() < 0.5:\n",
    "                image, targets = horisontal_flip(image, targets)\n",
    "\n",
    "        return image_path, image, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        paths, images, targets = list(zip(*batch))\n",
    "\n",
    "        # Remove empty placeholder targets\n",
    "        targets = [boxes for boxes in targets if boxes is not None]\n",
    "\n",
    "        # Add sample index to targets\n",
    "        for i, boxes in enumerate(targets):\n",
    "            boxes[:, 0] = i\n",
    "\n",
    "        try:\n",
    "            targets = torch.cat(targets, 0)\n",
    "        except RuntimeError:\n",
    "            targets = None  # No boxes for an image\n",
    "\n",
    "        # Selects new image size every 10 batches\n",
    "        if self.multiscale and self.batch_count % 10 == 0:\n",
    "            self.image_size = random.choice(range(320, 608 + 1, 32))\n",
    "\n",
    "        # Resize images to input shape\n",
    "        images = torch.stack([resize(image, self.image_size) for image in images])\n",
    "        self.batch_count += 1\n",
    "\n",
    "        return paths, images, targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
