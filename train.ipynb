{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from YOLOv3.ipynb\n"
     ]
    }
   ],
   "source": [
    "import easydict\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.utils.tensorboard\n",
    "import torch.nn as nn\n",
    "#import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "  \n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ipynb파일을 import할 수 있게 해주는 모듈\n",
    "# pip install import_ipynb\n",
    "# 에러가 뜨면 파일 저장 후 커널 restart\n",
    "import import_ipynb\n",
    "import YOLOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_config(path: str):\n",
    "    print(path)\n",
    "    \"\"\"데이터셋 설정 파일 분석\"\"\"\n",
    "    options = {}\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        key, value = line.split('=')\n",
    "        options[key.strip()] = value.strip()\n",
    "    return options\n",
    "\n",
    "def load_classes(path: str):\n",
    "    print(path)\n",
    "    \"\"\"클래스 이름 로드\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        names = f.readlines()\n",
    "    for i, name in enumerate(names):\n",
    "        names[i] = name.strip()\n",
    "    return names\n",
    "\n",
    "def init_weights_normal(m):\n",
    "    \"\"\"정규분포 형태로 가중치 초기화\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    # https://discuss.pytorch.org/t/object-has-no-attribute-weight/31526\n",
    "    # if classname.find(\"Conv\") != -1:\n",
    "    if type(m) == nn.Conv2d:\n",
    "        torch.nn.init.kaiming_normal_(m.weight.data, 0.1)\n",
    "\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, path, iou_thres, conf_thres, nms_thres, image_size, batch_size, num_workers, device):\n",
    "    # 모델을 evaluation mode로 설정\n",
    "    model.eval()\n",
    "\n",
    "    # 데이터셋, 데이터로더 설정\n",
    "    dataset = utils.datasets.ListDataset(path, image_size, augment=False, multiscale=False)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=num_workers,\n",
    "                                             collate_fn=dataset.collate_fn)\n",
    "\n",
    "    labels = []\n",
    "    sample_metrics = []  # List[Tuple] -> [(TP, confs, pred)]\n",
    "    entire_time = 0\n",
    "    for _, images, targets in tqdm.tqdm(dataloader, desc='Evaluate method', leave=False):\n",
    "        if targets is None:\n",
    "            continue\n",
    "\n",
    "        # Extract labels\n",
    "        labels.extend(targets[:, 1].tolist())\n",
    "\n",
    "        # Rescale targets\n",
    "        targets[:, 2:] = utils.utils.xywh2xyxy(targets[:, 2:])\n",
    "        targets[:, 2:] *= image_size\n",
    "\n",
    "        # Predict objects\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = utils.utils.non_max_suppression(outputs, conf_thres, nms_thres)\n",
    "        entire_time += time.time() - start_time\n",
    "\n",
    "        # Compute true positives, predicted scores and predicted labels per batch\n",
    "        sample_metrics.extend(utils.utils.get_batch_statistics(outputs, targets, iou_thres))\n",
    "\n",
    "    # Concatenate sample statistics\n",
    "    if len(sample_metrics) == 0:\n",
    "        true_positives, pred_scores, pred_labels = np.array([]), np.array([]), np.array([])\n",
    "    else:\n",
    "        true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
    "\n",
    "    # Compute AP\n",
    "    precision, recall, AP, f1, ap_class = utils.utils.ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
    "\n",
    "    # Compute inference time and fps\n",
    "    inference_time = entire_time / dataset.__len__()\n",
    "    fps = 1 / inference_time\n",
    "\n",
    "    # Export inference time to miliseconds\n",
    "    inference_time *= 1000\n",
    "\n",
    "    return precision, recall, AP, f1, ap_class, inference_time, fps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horisontal_flip(images, targets):\n",
    "    images = torch.flip(images, [-1])\n",
    "    targets[:, 2] = 1 - targets[:, 2]\n",
    "    return images, targets\n",
    "\n",
    "def pad_to_square(image, pad_value=0):\n",
    "    _, h, w = image.shape\n",
    "\n",
    "    # 너비와 높이의 차\n",
    "    difference = abs(h - w)\n",
    "\n",
    "    # (top, bottom) padding or (left, right) padding\n",
    "    if h <= w:\n",
    "        top = difference // 2\n",
    "        bottom = difference - difference // 2\n",
    "        pad = [0, 0, top, bottom]\n",
    "    else:\n",
    "        left = difference // 2\n",
    "        right = difference - difference // 2\n",
    "        pad = [left, right, 0, 0]\n",
    "\n",
    "    # Add padding\n",
    "    image = F.pad(image, pad, mode='constant', value=pad_value)\n",
    "    return image, pad\n",
    "\n",
    "def resize(image, size):\n",
    "    return F.interpolate(image.unsqueeze(0), size, mode='bilinear', align_corners=True).squeeze(0)\n",
    "\n",
    "\n",
    "class ImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path, image_size):\n",
    "        self.image_files = sorted(glob.glob(\"{}/*.*\".format(folder_path)))\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_files[index]\n",
    "\n",
    "        # Extract image as PyTorch tensor\n",
    "        image = torchvision.transforms.ToTensor()(Image.open(image_path).convert('RGB'))\n",
    "\n",
    "        # Pad to square resolution\n",
    "        image, _ = pad_to_square(image)\n",
    "\n",
    "        # Resize\n",
    "        image = resize(image, self.image_size)\n",
    "        return image_path, image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "\n",
    "\n",
    "class ListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_path: str, image_size: int, augment: bool, multiscale: bool, normalized_labels=True):\n",
    "        with open(list_path, 'r') as file:\n",
    "            self.image_files = file.readlines()\n",
    "\n",
    "        self.label_files = [path.replace('images', 'labels').replace('.png', '.txt').replace('.jpg', '.txt')\n",
    "                                .replace('JPEGImages', 'labels') for path in self.image_files]\n",
    "        self.image_size = image_size\n",
    "        self.max_objects = 100\n",
    "        self.augment = augment\n",
    "        self.multiscale = multiscale\n",
    "        self.normalized_labels = normalized_labels\n",
    "        self.batch_count = 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Image\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        image_path = self.image_files[index].rstrip()\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "            transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ColorJitter(brightness=1.5, saturation=1.5, hue=0.1),\n",
    "                torchvision.transforms.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            transforms = torchvision.transforms.ToTensor()\n",
    "\n",
    "        # Extract image as PyTorch tensor\n",
    "        image = transforms(Image.open(image_path).convert('RGB'))\n",
    "\n",
    "        _, h, w = image.shape\n",
    "        h_factor, w_factor = (h, w) if self.normalized_labels else (1, 1)\n",
    "\n",
    "        # Pad to square resolution\n",
    "        image, pad = pad_to_square(image)\n",
    "        _, padded_h, padded_w = image.shape\n",
    "\n",
    "        # 2. Label\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        label_path = self.label_files[index].rstrip()\n",
    "\n",
    "        targets = None\n",
    "        if os.path.exists(label_path):\n",
    "            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n",
    "\n",
    "            # Extract coordinates for unpadded + unscaled image\n",
    "            x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\n",
    "            y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\n",
    "            x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\n",
    "            y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\n",
    "\n",
    "            # Adjust for added padding\n",
    "            x1 += pad[0]\n",
    "            y1 += pad[2]\n",
    "            x2 += pad[1]\n",
    "            y2 += pad[3]\n",
    "\n",
    "            # Returns (x, y, w, h)\n",
    "            boxes[:, 1] = ((x1 + x2) / 2) / padded_w\n",
    "            boxes[:, 2] = ((y1 + y2) / 2) / padded_h\n",
    "            boxes[:, 3] *= w_factor / padded_w\n",
    "            boxes[:, 4] *= h_factor / padded_h\n",
    "\n",
    "            targets = torch.zeros((len(boxes), 6))\n",
    "            targets[:, 1:] = boxes\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "            if np.random.random() < 0.5:\n",
    "                image, targets = horisontal_flip(image, targets)\n",
    "\n",
    "        return image_path, image, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        paths, images, targets = list(zip(*batch))\n",
    "\n",
    "        # Remove empty placeholder targets\n",
    "        targets = [boxes for boxes in targets if boxes is not None]\n",
    "\n",
    "        # Add sample index to targets\n",
    "        for i, boxes in enumerate(targets):\n",
    "            boxes[:, 0] = i\n",
    "\n",
    "        try:\n",
    "            targets = torch.cat(targets, 0)\n",
    "        except RuntimeError:\n",
    "            targets = None  # No boxes for an image\n",
    "\n",
    "        # Selects new image size every 10 batches\n",
    "        if self.multiscale and self.batch_count % 10 == 0:\n",
    "            self.image_size = random.choice(range(320, 608 + 1, 32))\n",
    "\n",
    "        # Resize images to input shape\n",
    "        images = torch.stack([resize(image, self.image_size) for image in images])\n",
    "        self.batch_count += 1\n",
    "\n",
    "        return paths, images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'gradient_accumulation': 1, 'multiscale_training': True, 'batch_size': 32, 'num_workers': 0, 'data_config': 'config/coco.data', 'pretrained_weights': 'weights/darknet53.conv.74', 'image_size': 416}\n",
      "config/coco.data\n",
      "../../data/coco/coco_classes.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:   0%|                                                                                     | 0/1 [00:00<?, ?it/s]\n",
      "Batch:   0%|                                                                                     | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature map1 shape : torch.Size([3, 256, 68, 68])\n",
      "feature map2 shape : torch.Size([3, 512, 34, 34])\n",
      "feature map3 shape : torch.Size([3, 1024, 17, 17])\n",
      "1 ok\n",
      "2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|                                                                                     | 0/1 [00:03<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [3, 3, 17, 17] at index 0 does not match the shape of the indexed tensor [1, 3, 1, 1] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1d10ef067667>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# 순전파 (forward), 역전파 (backward)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Object_detect_study\\YOLOv3\\YOLOv3.ipynb\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, targets)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Object_detect_study\\YOLOv3\\YOLOv3.ipynb\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, targets)\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [3, 3, 17, 17] at index 0 does not match the shape of the indexed tensor [1, 3, 1, 1] at index 0"
     ]
    }
   ],
   "source": [
    "# 다양한 입력 매개변수를 분석한다. 아니면 기본 매개변수를 사용한다\n",
    "# ipynb에서는 argparse를 사용할 수 없다. easydict로 대체할 쉬 있다. \n",
    "# https://worthpreading.tistory.com/56\n",
    "args = easydict.EasyDict({\n",
    "    \"epoch\": 1,\n",
    "    \"gradient_accumulation\": 1,\n",
    "    \"multiscale_training\": True,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"data_config\": \"config/coco.data\",\n",
    "    \"pretrained_weights\": 'weights/darknet53.conv.74',\n",
    "    \"image_size\": 416\n",
    "})\n",
    "print(args)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "now = time.strftime('%y%m%d_%H%M%S', time.localtime(time.time()))\n",
    "\n",
    "# Tensorboard writer 객체 생성\n",
    "log_dir = os.path.join('logs', now)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = torch.utils.tensorboard.SummaryWriter(log_dir)\n",
    "\n",
    "# 데이터셋 설정값을 가져오기\n",
    "data_config = parse_data_config(args.data_config)\n",
    "train_path = data_config['train']\n",
    "valid_path = data_config['valid']\n",
    "num_classes = int(data_config['classes'])\n",
    "class_names = load_classes(data_config['names'])\n",
    "\n",
    "\n",
    "# # 모델 준비하기\n",
    "model = YOLOv3.YOLOv3(args.image_size, num_classes).to(device)\n",
    "model.apply(init_weights_normal)\n",
    "# if args.pretrained_weights.endswith('.pth'):\n",
    "#     model.load_state_dict(torch.load(args.pretrained_weights))\n",
    "# else:\n",
    "#     model.load_darknet_weights(args.pretrained_weights)\n",
    "\n",
    "dataset = ListDataset(train_path, args.image_size, augment=True, multiscale=args.multiscale_training)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=args.batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args.num_workers,\n",
    "                                         pin_memory=True,\n",
    "                                         collate_fn=dataset.collate_fn)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# learning rate scheduler 설정\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "\n",
    "# 현재 배치 손실값을 출력하는 tqdm 설정\n",
    "loss_log = tqdm(total=0, position=2, bar_format='{desc}', leave=False)\n",
    "\n",
    "# Train code.\n",
    "for epoch in tqdm(range(args.epoch), desc='Epoch'):\n",
    "    # 모델을 train mode로 설정\n",
    "    model.train()\n",
    "    # 1 epoch의 각 배치에서 처리하는 코드\n",
    "    for batch_idx, (_, images, targets) in enumerate(tqdm(dataloader, desc='Batch', leave=False)):\n",
    "        step = len(dataloader) * epoch + batch_idx\n",
    "\n",
    "        # 이미지와 정답 정보를 GPU로 복사\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 (forward), 역전파 (backward)\n",
    "        loss, outputs = model(images, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # 기울기 누적 (Accumulate gradient)\n",
    "        if step % args.gradient_accumulation == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # 총 손실값 출력\n",
    "        loss_log.set_description_str('Loss: {:.6f}'.format(loss.item()))\n",
    "\n",
    "        # Tensorboard에 훈련 과정 기록\n",
    "        tensorboard_log = []\n",
    "        for i, yolo_layer in enumerate(model.yolo_layers):\n",
    "            writer.add_scalar('loss_bbox_{}'.format(i + 1), yolo_layer.metrics['loss_bbox'], step)\n",
    "            writer.add_scalar('loss_conf_{}'.format(i + 1), yolo_layer.metrics['loss_conf'], step)\n",
    "            writer.add_scalar('loss_cls_{}'.format(i + 1), yolo_layer.metrics['loss_cls'], step)\n",
    "            writer.add_scalar('loss_layer_{}'.format(i + 1), yolo_layer.metrics['loss_layer'], step)\n",
    "        writer.add_scalar('total_loss', loss.item(), step)\n",
    "\n",
    "    # lr scheduler의 step을 진행\n",
    "    scheduler.step()\n",
    "\n",
    "    # 검증 데이터셋으로 모델을 평가\n",
    "    precision, recall, AP, f1, _, _, _ = evaluate(model,\n",
    "                                                  path=valid_path,\n",
    "                                                  iou_thres=0.5,\n",
    "                                                  conf_thres=0.5,\n",
    "                                                  nms_thres=0.5,\n",
    "                                                  image_size=args.image_size,\n",
    "                                                  batch_size=args.batch_size,\n",
    "                                                  num_workers=args.num_workers,\n",
    "                                                  device=device)\n",
    "\n",
    "    # Tensorboard에 평가 결과 기록\n",
    "    writer.add_scalar('val_precision', precision.mean(), epoch)\n",
    "    writer.add_scalar('val_recall', recall.mean(), epoch)\n",
    "    writer.add_scalar('val_mAP', AP.mean(), epoch)\n",
    "    writer.add_scalar('val_f1', f1.mean(), epoch)\n",
    "\n",
    "    # checkpoint file 저장\n",
    "    save_dir = os.path.join('checkpoints', now)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    dataset_name = os.path.split(args.data_config)[-1].split('.')[0]\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'yolov3_{}_{}.pth'.format(dataset_name, epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
