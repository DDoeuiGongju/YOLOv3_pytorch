{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.utils.tensorboard\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "  \n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ipynb파일을 import할 수 있게 해주는 모듈\n",
    "# pip install import_ipynb\n",
    "# 에러가 뜨면 파일 저장 후 커널 restart\n",
    "import import_ipynb\n",
    "import YOLOv3\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "def horisontal_flip(images, targets):\n",
    "    images = torch.flip(images, [-1])\n",
    "    targets[:, 2] = 1 - targets[:, 2]\n",
    "    return images, targets\n",
    "\n",
    "def pad_to_square(image, pad_value=0):\n",
    "    _, h, w = image.shape\n",
    "\n",
    "    # 너비와 높이의 차\n",
    "    difference = abs(h - w)\n",
    "\n",
    "    # (top, bottom) padding or (left, right) padding\n",
    "    if h <= w:\n",
    "        top = difference // 2\n",
    "        bottom = difference - difference // 2\n",
    "        pad = [0, 0, top, bottom]\n",
    "    else:\n",
    "        left = difference // 2\n",
    "        right = difference - difference // 2\n",
    "        pad = [left, right, 0, 0]\n",
    "\n",
    "    # Add padding\n",
    "    image = F.pad(image, pad, mode='constant', value=pad_value)\n",
    "    return image, pad\n",
    "\n",
    "def resize(image, size):\n",
    "    return F.interpolate(image.unsqueeze(0), size, mode='bilinear', align_corners=True).squeeze(0)\n",
    "\n",
    "\n",
    "class ImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path, image_size):\n",
    "        self.image_files = sorted(glob.glob(\"{}/*.*\".format(folder_path)))\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_files[index]\n",
    "\n",
    "        # Extract image as PyTorch tensor\n",
    "        image = torchvision.transforms.ToTensor()(Image.open(image_path).convert('RGB'))\n",
    "\n",
    "        # Pad to square resolution\n",
    "        image, _ = pad_to_square(image)\n",
    "\n",
    "        # Resize\n",
    "        image = resize(image, self.image_size)\n",
    "        return image_path, image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "\n",
    "\n",
    "class ListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_path: str, image_size: int, augment: bool, multiscale: bool, normalized_labels=True):\n",
    "        with open(list_path, 'r') as file:\n",
    "            self.image_files = file.readlines()\n",
    "\n",
    "        self.label_files = [path.replace('images', 'labels').replace('.png', '.txt').replace('.jpg', '.txt')\n",
    "                                .replace('JPEGImages', 'labels') for path in self.image_files]\n",
    "        self.image_size = image_size\n",
    "        self.max_objects = 100\n",
    "        self.augment = augment\n",
    "        self.multiscale = multiscale\n",
    "        self.normalized_labels = normalized_labels\n",
    "        self.batch_count = 0\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Image\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        image_path = self.image_files[index].rstrip()\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "            transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ColorJitter(brightness=1.5, saturation=1.5, hue=0.1),\n",
    "                torchvision.transforms.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            transforms = torchvision.transforms.ToTensor()\n",
    "\n",
    "        # Extract image as PyTorch tensor\n",
    "        image = transforms(Image.open(image_path).convert('RGB'))\n",
    "\n",
    "        _, h, w = image.shape\n",
    "        h_factor, w_factor = (h, w) if self.normalized_labels else (1, 1)\n",
    "\n",
    "        # Pad to square resolution\n",
    "        image, pad = pad_to_square(image)\n",
    "        _, padded_h, padded_w = image.shape\n",
    "\n",
    "        # 2. Label\n",
    "        # -----------------------------------------------------------------------------------\n",
    "        label_path = self.label_files[index].rstrip()\n",
    "\n",
    "        targets = None\n",
    "        if os.path.exists(label_path):\n",
    "            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n",
    "\n",
    "            # Extract coordinates for unpadded + unscaled image\n",
    "            x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\n",
    "            y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\n",
    "            x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\n",
    "            y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\n",
    "\n",
    "            # Adjust for added padding\n",
    "            x1 += pad[0]\n",
    "            y1 += pad[2]\n",
    "            x2 += pad[1]\n",
    "            y2 += pad[3]\n",
    "\n",
    "            # Returns (x, y, w, h)\n",
    "            boxes[:, 1] = ((x1 + x2) / 2) / padded_w\n",
    "            boxes[:, 2] = ((y1 + y2) / 2) / padded_h\n",
    "            boxes[:, 3] *= w_factor / padded_w\n",
    "            boxes[:, 4] *= h_factor / padded_h\n",
    "\n",
    "            targets = torch.zeros((len(boxes), 6))\n",
    "            targets[:, 1:] = boxes\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.augment:\n",
    "            if np.random.random() < 0.5:\n",
    "                image, targets = horisontal_flip(image, targets)\n",
    "\n",
    "        return image_path, image, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        paths, images, targets = list(zip(*batch))\n",
    "\n",
    "        # Remove empty placeholder targets\n",
    "        targets = [boxes for boxes in targets if boxes is not None]\n",
    "\n",
    "        # Add sample index to targets\n",
    "        for i, boxes in enumerate(targets):\n",
    "            boxes[:, 0] = i\n",
    "\n",
    "        try:\n",
    "            targets = torch.cat(targets, 0)\n",
    "        except RuntimeError:\n",
    "            targets = None  # No boxes for an image\n",
    "\n",
    "        # Selects new image size every 10 batches\n",
    "        if self.multiscale and self.batch_count % 10 == 0:\n",
    "            self.image_size = random.choice(range(320, 608 + 1, 32))\n",
    "\n",
    "        # Resize images to input shape\n",
    "        images = torch.stack([resize(image, self.image_size) for image in images])\n",
    "        self.batch_count += 1\n",
    "\n",
    "        return paths, images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, path, iou_thres, conf_thres, nms_thres, image_size, batch_size, num_workers, device):\n",
    "    # 모델을 evaluation mode로 설정\n",
    "    model.eval()\n",
    "\n",
    "    # 데이터셋, 데이터로더 설정\n",
    "    dataset = ListDataset(path, image_size, augment=False, multiscale=False)   # utils.datasets.ListDatase\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=num_workers,\n",
    "                                             collate_fn=dataset.collate_fn)\n",
    "\n",
    "    labels = []\n",
    "    sample_metrics = []  # List[Tuple] -> [(TP, confs, pred)]\n",
    "    entire_time = 0\n",
    "    for _, images, targets in tqdm.tqdm(dataloader, desc='Evaluate method', leave=False):\n",
    "        if targets is None:\n",
    "            continue\n",
    "\n",
    "        # Extract labels\n",
    "        labels.extend(targets[:, 1].tolist())\n",
    "\n",
    "        # Rescale targets\n",
    "        targets[:, 2:] = utils.xywh2xyxy(targets[:, 2:])\n",
    "        targets[:, 2:] *= image_size\n",
    "\n",
    "        # Predict objects\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            outputs = utils.non_max_suppression(outputs, conf_thres, nms_thres)\n",
    "        entire_time += time.time() - start_time\n",
    "\n",
    "        # Compute true positives, predicted scores and predicted labels per batch\n",
    "        sample_metrics.extend(utils.get_batch_statistics(outputs, targets, iou_thres))\n",
    "\n",
    "    # Concatenate sample statistics\n",
    "    if len(sample_metrics) == 0:\n",
    "        true_positives, pred_scores, pred_labels = np.array([]), np.array([]), np.array([])\n",
    "    else:\n",
    "        true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
    "\n",
    "    # Compute AP\n",
    "    precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
    "\n",
    "    # Compute inference time and fps\n",
    "    inference_time = entire_time / dataset.__len__()\n",
    "    fps = 1 / inference_time\n",
    "\n",
    "    # Export inference time to miliseconds\n",
    "    inference_time *= 1000\n",
    "\n",
    "    return precision, recall, AP, f1, ap_class, inference_time, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'gradient_accumulation': 1, 'multiscale_training': True, 'batch_size': 32, 'num_workers': 0, 'data_config': 'config/coco.data', 'pretrained_weights': 'weights/darknet53.conv.74', 'image_size': 416}\n",
      "config/coco.data\n",
      "../../data/coco/coco_classes.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:   0%|                                                                                     | 0/1 [00:00<?, ?it/s]\n",
      "Batch:   0%|                                                                                     | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.83s/it]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Evaluate method:   0%|                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 272.923401\n",
      "--------------------------------------\n",
      "loss_bbox_1 5.644756317138672 0\n",
      "loss_conf_1 86.14558410644531 0\n",
      "loss_cls_1 0.7585853934288025 0\n",
      "loss_layer_1 92.5489273071289 0\n",
      "--------------------------------------\n",
      "loss_bbox_2 4.698484897613525 0\n",
      "loss_conf_2 91.67838287353516 0\n",
      "loss_cls_2 0.78904789686203 0\n",
      "loss_layer_2 97.1659164428711 0\n",
      "--------------------------------------\n",
      "loss_bbox_3 2.083251476287842 0\n",
      "loss_conf_3 80.32605743408203 0\n",
      "loss_cls_3 0.7992402911186218 0\n",
      "loss_layer_3 83.20854949951172 0\n",
      "\n",
      "total_loss 272.92340087890625 0 \n",
      "\n",
      "1 ok\n",
      "2 ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "Epoch:   0%|                                                                                     | 0/1 [05:51<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8276d29bade3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2 ok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;31m# 검증 데이터셋으로 모델을 평가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     precision, recall, AP, f1, _, _, _ = evaluate(model,\n\u001b[0m\u001b[0;32m    108\u001b[0m                                                   \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                                                   \u001b[0miou_thres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-eea5eebcb2d2>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, path, iou_thres, conf_thres, nms_thres, image_size, batch_size, num_workers, device)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnon_max_suppression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnms_thres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mentire_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Object_detect_study\\YOLOv3\\utils.ipynb\u001b[0m in \u001b[0;36mnon_max_suppression\u001b[1;34m(prediction, conf_thres, nms_thres)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 다양한 입력 매개변수를 분석한다. 아니면 기본 매개변수를 사용한다\n",
    "# ipynb에서는 argparse를 사용할 수 없다. easydict로 대체할 쉬 있다. \n",
    "# https://worthpreading.tistory.com/56\n",
    "args = easydict.EasyDict({\n",
    "    \"epoch\": 1,\n",
    "    \"gradient_accumulation\": 1,\n",
    "    \"multiscale_training\": True,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"data_config\": \"config/coco.data\",\n",
    "    \"pretrained_weights\": 'weights/darknet53.conv.74',\n",
    "    \"image_size\": 416\n",
    "})\n",
    "print(args)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "now = time.strftime('%y%m%d_%H%M%S', time.localtime(time.time()))\n",
    "\n",
    "# Tensorboard writer 객체 생성\n",
    "log_dir = os.path.join('logs', now)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = torch.utils.tensorboard.SummaryWriter(log_dir)\n",
    "\n",
    "# 데이터셋 설정값을 가져오기\n",
    "data_config = utils.parse_data_config(args.data_config)\n",
    "train_path = data_config['train']\n",
    "valid_path = data_config['valid']\n",
    "num_classes = int(data_config['classes'])\n",
    "class_names = utils.load_classes(data_config['names'])\n",
    "\n",
    "\n",
    "# # 모델 준비하기\n",
    "model = YOLOv3.YOLOv3(args.image_size, num_classes).to(device)\n",
    "model.apply(utils.init_weights_normal)\n",
    "# if args.pretrained_weights.endswith('.pth'):\n",
    "#     model.load_state_dict(torch.load(args.pretrained_weights))\n",
    "# else:\n",
    "#     model.load_darknet_weights(args.pretrained_weights)\n",
    "\n",
    "dataset = ListDataset(train_path, args.image_size, augment=True, multiscale=args.multiscale_training)\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=args.batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args.num_workers,\n",
    "                                         pin_memory=True,\n",
    "                                         collate_fn=dataset.collate_fn)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# learning rate scheduler 설정\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "\n",
    "# 현재 배치 손실값을 출력하는 tqdm 설정\n",
    "loss_log = tqdm.tqdm(total=0, position=2, bar_format='{desc}', leave=False)\n",
    "\n",
    "# Train code.\n",
    "for epoch in tqdm.tqdm(range(args.epoch), desc='Epoch'):\n",
    "    print(epoch)\n",
    "    # 모델을 train mode로 설정\n",
    "    model.train()\n",
    "    # 1 epoch의 각 배치에서 처리하는 코드\n",
    "    for batch_idx, (_, images, targets) in enumerate(tqdm.tqdm(dataloader, desc='Batch', leave=False)):\n",
    "        step = len(dataloader) * epoch + batch_idx\n",
    "\n",
    "        # 이미지와 정답 정보를 GPU로 복사\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 (forward), 역전파 (backward)\n",
    "        loss, outputs = model(images, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 기울기 누적 (Accumulate gradient)\n",
    "        if step % args.gradient_accumulation == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "#         # 총 손실값 출력\n",
    "#         loss_log.set_description_str('Loss: {:.6f}'.format(loss.item()))\n",
    "\n",
    "#         # Tensorboard에 훈련 과정 기록\n",
    "#         tensorboard_log = []\n",
    "#         for i, yolo_layer in enumerate(model.yolo_layers):\n",
    "#             writer.add_scalar('loss_bbox_{}'.format(i + 1), yolo_layer.metrics['loss_bbox'], step)\n",
    "#             writer.add_scalar('loss_conf_{}'.format(i + 1), yolo_layer.metrics['loss_conf'], step)\n",
    "#             writer.add_scalar('loss_cls_{}'.format(i + 1), yolo_layer.metrics['loss_cls'], step)\n",
    "#             writer.add_scalar('loss_layer_{}'.format(i + 1), yolo_layer.metrics['loss_layer'], step)\n",
    "#         writer.add_scalar('total_loss', loss.item(), step)\n",
    "        \n",
    "        print('Loss: {:.6f}'.format(loss.item()))\n",
    "        \n",
    "        for i, yolo_layer in enumerate(model.yolo_layers):\n",
    "            print('--------------------------------------')\n",
    "            print('loss_bbox_{}'.format(i + 1), yolo_layer.metrics['loss_bbox'], step)\n",
    "            print('loss_conf_{}'.format(i + 1), yolo_layer.metrics['loss_conf'], step)\n",
    "            print('loss_cls_{}'.format(i + 1), yolo_layer.metrics['loss_cls'], step)\n",
    "            print('loss_layer_{}'.format(i + 1), yolo_layer.metrics['loss_layer'], step)\n",
    "            \n",
    "        print('\\ntotal_loss', loss.item(), step, '\\n')\n",
    "\n",
    "    # lr scheduler의 step을 진행\n",
    "    scheduler.step()\n",
    "\n",
    "    # 검증 데이터셋으로 모델을 평가\n",
    "    precision, recall, AP, f1, _, _, _ = evaluate(model,\n",
    "                                                  path=valid_path,\n",
    "                                                  iou_thres=0.5,\n",
    "                                                  conf_thres=0.5,\n",
    "                                                  nms_thres=0.5,\n",
    "                                                  image_size=args.image_size,\n",
    "                                                  batch_size=args.batch_size,\n",
    "                                                  num_workers=args.num_workers,\n",
    "                                                  device=device)\n",
    "#     # Tensorboard에 평가 결과 기록\n",
    "#     writer.add_scalar('val_precision', precision.mean(), epoch)\n",
    "#     writer.add_scalar('val_recall', recall.mean(), epoch)\n",
    "#     writer.add_scalar('val_mAP', AP.mean(), epoch)\n",
    "#     writer.add_scalar('val_f1', f1.mean(), epoch)\n",
    "\n",
    "    print('val_precision', precision.mean(), epoch)\n",
    "    print('val_recall', recall.mean(), epoch)\n",
    "    print('val_mAP', AP.mean(), epoch)\n",
    "    print('val_f1', f1.mean(), epoch)\n",
    "\n",
    "    # checkpoint file 저장\n",
    "    save_dir = os.path.join('checkpoints', now)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    dataset_name = os.path.split(args.data_config)[-1].split('.')[0]\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'yolov3_{}_{}.pth'.format(dataset_name, epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
